{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing gait detection\n",
    "This script preprocesses the sensor data, tabulates it into windows, and generates features. \n",
    "\n",
    "Execution time $\\approx$ 1 minute per participant (amounting to 45 minutes in total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# # if you are unable to load pdathome.constants, you need to add the path to the src folder to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from pdathome.constants import columns, descriptives, participant_ids, paths\n",
    "from pdathome.preprocessing import preprocess_gait\n",
    "from pdathome.utils import in_notebook\n",
    "\n",
    "from paradigma.feature_extraction import extract_temporal_domain_features, extract_spectral_domain_features\n",
    "from paradigma.imu_preprocessing import butterworth_filter\n",
    "from paradigma.gait_analysis_config import GaitFeatureExtractionConfig\n",
    "from paradigma.preprocessing_config import IMUPreprocessingConfig\n",
    "from paradigma.windowing import tabulate_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '--f=c:\\\\Users\\\\erik_\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v38e747054cdf66600364b449df76fdb0d7468c4db.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a list of (subject, side) tuples\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [(subject, side) \u001b[38;5;28;01mfor\u001b[39;00m subject \u001b[38;5;129;01min\u001b[39;00m l_ids \u001b[38;5;28;01mfor\u001b[39;00m side \u001b[38;5;129;01min\u001b[39;00m [descriptives\u001b[38;5;241m.\u001b[39mMOST_AFFECTED_SIDE, descriptives\u001b[38;5;241m.\u001b[39mLEAST_AFFECTED_SIDE]]\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnproc\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Use map to pass the list of (subject, side) tuples\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         results \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmap(pooling_func, tasks)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '--f=c:\\\\Users\\\\erik_\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v38e747054cdf66600364b449df76fdb0d7468c4db.json'"
     ]
    }
   ],
   "source": [
    "def pooling_func(subject_side_tuple):\n",
    "    subject, side = subject_side_tuple\n",
    "    preprocess_gait(subject, side)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nproc = sys.argv[1]  # Number of processors\n",
    "    l_ids = sys.argv[2:]  # List of subjects\n",
    "\n",
    "    # Create a list of (subject, side) tuples\n",
    "    tasks = [(subject, side) for subject in l_ids for side in [descriptives.MOST_AFFECTED_SIDE, descriptives.LEAST_AFFECTED_SIDE]]\n",
    "\n",
    "    with Pool(int(nproc)) as p:\n",
    "        try:\n",
    "            # Use map to pass the list of (subject, side) tuples\n",
    "            results = p.map(pooling_func, tasks)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    for subject in participant_ids.L_PD_IDS + participant_ids.L_HC_IDS:\n",
    "        for side in [descriptives.MOST_AFFECTED_SIDE, descriptives.LEAST_AFFECTED_SIDE]:\n",
    "            preprocess_gait(subject, side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j, subject in enumerate(participant_ids.L_PD_IDS + participant_ids.L_HC_IDS, 1):        \n",
    "#     for side in [descriptives.MOST_AFFECTED_SIDE, descriptives.LEAST_AFFECTED_SIDE]:\n",
    "#         print(f\"Time {datetime.datetime.now()} - {subject} {side} - Processing ... ({j}/{len(participant_ids.L_PD_IDS + participant_ids.L_HC_IDS)})\")\n",
    "#         df = pd.read_pickle(os.path.join(paths.PATH_DATAFRAMES, f'{subject}_{side}.pkl'))\n",
    "\n",
    "#         config = IMUPreprocessingConfig()\n",
    "#         config.acceleration_units = 'g'\n",
    "\n",
    "#         # Extract relevant columns for accelerometer data\n",
    "#         accel_cols = list(config.d_channels_accelerometer.keys())\n",
    "\n",
    "#         # Change to correct units [g]\n",
    "#         df[accel_cols] = df[accel_cols] / 9.81 if config.acceleration_units == 'm/s^2' else df[accel_cols]\n",
    "\n",
    "#         # Extract the accelerometer data as a 2D array\n",
    "#         accel_data = df[accel_cols].values\n",
    "\n",
    "#         # Define filtering passbands\n",
    "#         passbands = ['hp', 'lp'] \n",
    "#         filtered_data = {}\n",
    "\n",
    "#         # Apply Butterworth filter for each passband and result type\n",
    "#         for result, passband in zip(['filt', 'grav'], passbands):\n",
    "#             filtered_data[result] = butterworth_filter(\n",
    "#                 sensor_data=accel_data,\n",
    "#                 order=config.filter_order,\n",
    "#                 cutoff_frequency=config.lower_cutoff_frequency,\n",
    "#                 passband=passband,\n",
    "#                 sampling_frequency=config.sampling_frequency\n",
    "#             )\n",
    "\n",
    "#         # Create DataFrames from filtered data\n",
    "#         filtered_dfs = {f'{result}_{col}': pd.Series(data[:, i]) for i, col in enumerate(accel_cols) for result, data in filtered_data.items()}\n",
    "\n",
    "#         # Combine filtered columns into DataFrame\n",
    "#         filtered_df = pd.DataFrame(filtered_dfs)\n",
    "\n",
    "#         # Drop original accelerometer columns and append filtered results\n",
    "#         df = df.drop(columns=accel_cols).join(filtered_df).rename(columns={col: col.replace('filt_', '') for col in filtered_df.columns})\n",
    "\n",
    "#         config = GaitFeatureExtractionConfig()\n",
    "\n",
    "#         config.l_data_point_level_cols += [config.time_colname, columns.FREE_LIVING_LABEL]\n",
    "#         l_ts_cols = [config.time_colname, columns.WINDOW_NR, columns.FREE_LIVING_LABEL]\n",
    "#         l_export_cols = [config.time_colname, columns.WINDOW_NR, columns.ACTIVITY_LABEL_MAJORITY_VOTING, columns.GAIT_MAJORITY_VOTING] + list(config.d_channels_values.keys())\n",
    "\n",
    "#         if subject in participant_ids.L_PD_IDS:\n",
    "#             config.l_data_point_level_cols += [columns.PRE_OR_POST, columns.ARM_LABEL]\n",
    "#             l_ts_cols += [columns.PRE_OR_POST, columns.ARM_LABEL]\n",
    "#             l_export_cols += [columns.PRE_OR_POST, columns.ARM_LABEL_MAJORITY_VOTING]\n",
    "#         if subject in participant_ids.L_TREMOR_IDS:\n",
    "#             config.l_data_point_level_cols += [columns.TREMOR_LABEL]\n",
    "#             l_ts_cols += [columns.TREMOR_LABEL]\n",
    "\n",
    "\n",
    "#         df_windowed = tabulate_windows(\n",
    "#                 df=df,\n",
    "#                 time_column_name=config.time_colname,\n",
    "#                 data_point_level_cols=config.l_data_point_level_cols,\n",
    "#                 window_length_s=config.window_length_s,\n",
    "#                 window_step_size_s=config.window_step_size_s,\n",
    "#                 sampling_frequency=config.sampling_frequency\n",
    "#         )\n",
    "        \n",
    "#         # store windows with timestamps for later use\n",
    "#         df_windowed[l_ts_cols].to_pickle(os.path.join(paths.PATH_GAIT_FEATURES, f'{subject}_{side}_ts.pkl'))\n",
    "\n",
    "#         # Determine most prevalent activity\n",
    "#         df_windowed[columns.ACTIVITY_LABEL_MAJORITY_VOTING] = df_windowed[columns.FREE_LIVING_LABEL].apply(lambda x: pd.Series(x).mode()[0])\n",
    "\n",
    "#         # Determine if the majority of the window is walking\n",
    "#         df_windowed[columns.GAIT_MAJORITY_VOTING] = df_windowed[columns.FREE_LIVING_LABEL].apply(lambda x: x.count('Walking') >= len(x)/2)\n",
    "\n",
    "#         if subject in participant_ids.L_PD_IDS:\n",
    "#             df_windowed[columns.PRE_OR_POST] = df_windowed[columns.PRE_OR_POST].str[0]\n",
    "#             df_windowed[columns.ARM_LABEL_MAJORITY_VOTING] = df_windowed[columns.ARM_LABEL].apply(lambda x: arm_label_majority_voting(config, x))\n",
    "\n",
    "#         df_windowed = df_windowed.drop(columns=[x for x in l_ts_cols if x not in [columns.WINDOW_NR, columns.PRE_OR_POST]])\n",
    "\n",
    "#         # compute statistics of the temporal domain signals\n",
    "#         df_windowed = extract_temporal_domain_features(\n",
    "#             config=config,\n",
    "#             df_windowed=df_windowed,\n",
    "#             l_gravity_stats=['mean', 'std']\n",
    "#         )\n",
    "\n",
    "#         # transform the signals from the temporal domain to the spectral domain using the fast fourier transform\n",
    "#         # and extract spectral features\n",
    "#         df_windowed = extract_spectral_domain_features(\n",
    "#             config=config,\n",
    "#             df_windowed=df_windowed,\n",
    "#             sensor=config.sensor,\n",
    "#             l_sensor_colnames=config.l_accelerometer_cols\n",
    "#         )\n",
    "\n",
    "#         df_windowed[l_export_cols].to_pickle(os.path.join(paths.PATH_GAIT_FEATURES, f'{subject}_{side}.pkl'))\n",
    "\n",
    "#     clear_output(wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait-3c0zjlKo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
