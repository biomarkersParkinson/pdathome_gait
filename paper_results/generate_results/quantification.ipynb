{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hbv002...\n",
      "Processing hbv012...\n",
      "Processing hbv014...\n",
      "Processing hbv015...\n",
      "Processing hbv016...\n",
      "Processing hbv017...\n",
      "Processing hbv022...\n",
      "Processing hbv024...\n",
      "Processing hbv039...\n",
      "Processing hbv043...\n",
      "Processing hbv047...\n",
      "Processing hbv054...\n",
      "Processing hbv065...\n",
      "Processing hbv077...\n",
      "Processing hbv079...\n",
      "Processing hbv090...\n",
      "Processing hbv013...\n",
      "Processing hbv018...\n",
      "Processing hbv023...\n",
      "Processing hbv038...\n",
      "Processing hbv058...\n",
      "Processing hbv063...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pdathome.constants import global_constants as gc, mappings as mp\n",
    "from paradigma.windowing import create_segments\n",
    "\n",
    "\n",
    "l_metrics = ['range_of_motion', 'peak_velocity']\n",
    "l_aggregates = ['median']\n",
    "l_quantiles = [0.95]\n",
    "\n",
    "l_as_measures = ['range_of_motion_median', 'range_of_motion_quantile_95', 'peak_velocity_median', 'peak_velocity_quantile_95']\n",
    "\n",
    "gd_model_selected = gc.classifiers.RANDOM_FOREST\n",
    "asd_model_selected = gc.classifiers.LOGISTIC_REGRESSION\n",
    "\n",
    "l_groupby = [gc.columns.PRE_OR_POST]\n",
    "l_groupby_side = l_groupby #+ ['affected_side']\n",
    "l_groupby_segment = l_groupby_side # + [gc.columns.SEGMENT_CAT]\n",
    "   \n",
    "\n",
    "def extract_features(df, l_groupby, l_metrics, l_aggregates, l_quantiles=[]):\n",
    "    # seperate quantiles from other aggregates\n",
    "    l_df_agg = []\n",
    "    for metric in l_metrics:\n",
    "        df_agg = df.groupby(l_groupby)[metric].agg(l_aggregates).reset_index().rename(columns={x: f'{metric}_{x}' for x in l_aggregates})\n",
    "        df_qs = df.groupby(l_groupby)[metric].quantile(l_quantiles).reset_index()\n",
    "\n",
    "        for quantile in l_quantiles:\n",
    "            df_agg[f\"{metric}_quantile_{int(quantile*100)}\"] = df_qs.loc[df_qs[f'level_{len(l_groupby)}']==quantile, metric].reset_index(drop=True) \n",
    "\n",
    "        l_df_agg.append(df_agg)\n",
    "\n",
    "    for j in range(len(l_df_agg)):\n",
    "        if j == 0:\n",
    "            df_agg = l_df_agg[j]\n",
    "        else:\n",
    "            df_agg = pd.merge(left=df_agg, right=l_df_agg[j], how='left', on=l_groupby)\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def compute_aggregations(df, l_measures):\n",
    "    df_agg_side = extract_features(df, l_groupby_side, l_metrics, l_aggregates, l_quantiles)\n",
    "\n",
    "    d_quant = {}\n",
    "\n",
    "    for med_stage in df_agg_side[gc.columns.PRE_OR_POST].unique():\n",
    "        d_quant[med_stage] = {\n",
    "            'seconds': {\n",
    "                'overall': df.loc[(df[gc.columns.PRE_OR_POST]==med_stage)].shape[0] / gc.parameters.DOWNSAMPLED_FREQUENCY,\n",
    "            },\n",
    "            'values': {}\n",
    "        }\n",
    "        for measure in l_measures:\n",
    "            d_quant[med_stage]['values'][measure] = {\n",
    "                'overall': df_agg_side.loc[(df_agg_side[gc.columns.PRE_OR_POST]==med_stage), measure].values[0]\n",
    "            }\n",
    "\n",
    "    df_agg_side = extract_features(df, l_groupby_segment, l_metrics, l_aggregates, l_quantiles)\n",
    "\n",
    "    for med_stage in df_agg_side[gc.columns.PRE_OR_POST].unique():\n",
    "        df_med_stage = df_agg_side.loc[df_agg_side[gc.columns.PRE_OR_POST]==med_stage]\n",
    "        # for segment_duration_category in df_med_stage[gc.columns.SEGMENT_CAT].unique():\n",
    "            # df_segment_duration = df_med_stage.loc[df_med_stage[gc.columns.SEGMENT_CAT]==segment_duration_category].copy().reset_index()\n",
    "            # d_quant[med_stage]['seconds'][segment_duration_category] = df.loc[(df[gc.columns.PRE_OR_POST]==med_stage) & (df[gc.columns.SEGMENT_CAT]==segment_duration_category)].shape[0] / fs\n",
    "            # for measure in l_measures:\n",
    "                # d_quant[med_stage]['values'][measure][segment_duration_category] = df_segment_duration[measure].values[0]\n",
    "\n",
    "\n",
    "        d_quant[med_stage]['seconds']['non_gait'] = d_quant[med_stage]['seconds']['overall']  - np.sum([d_quant[med_stage]['seconds'][segment_duration_category] for segment_duration_category in mp.segment_map.keys() if segment_duration_category in d_quant[med_stage]['seconds'].keys()])\n",
    "\n",
    "    return d_quant\n",
    "\n",
    "\n",
    "def compute_effect_size(df, measure, stat):\n",
    "    df_all_mas = df.loc[df[gc.columns.SIDE]==gc.descriptives.MOST_AFFECTED_SIDE].copy()\n",
    "    df_all_mas['dataset'] = 'Predicted gait'\n",
    "\n",
    "    df_pred_mas = df_all_mas.loc[(df_all_mas[gc.columns.PRED_OTHER_ARM_ACTIVITY]==0)].copy()\n",
    "    df_pred_mas['dataset'] = 'Predicted gait predicted NOAA'\n",
    "\n",
    "    df_ann_mas = df_all_mas.loc[(df_all_mas['other_arm_activity_boolean']==0)].copy()\n",
    "    df_ann_mas['dataset'] = 'Predicted gait annotated NOAA'\n",
    "\n",
    "    df_mas = pd.concat([df_all_mas, df_pred_mas], axis=0).reset_index(drop=True)\n",
    "    df_mas = pd.concat([df_mas, df_ann_mas], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # effect size: verschil (point estimate) gedeeld door de std van het verschil (sd van point estimate van bootstrapping)\n",
    "    d_effect_size = {}\n",
    "    d_diffs = {}\n",
    "\n",
    "    for dataset in df_mas['dataset'].unique():\n",
    "        d_effect_size[dataset] = {}\n",
    "\n",
    "        for segment_duration in ['overall']:# ['short', 'moderately_long', 'long', 'very_long', 'overall']:\n",
    "            df_pre = df_mas.loc[(df_mas['dataset']==dataset) & (df_mas[gc.columns.PRE_OR_POST]==gc.descriptives.PRE_MED)]\n",
    "            df_post = df_mas.loc[(df_mas['dataset']==dataset) & (df_mas[gc.columns.PRE_OR_POST]==gc.descriptives.POST_MED)]\n",
    "\n",
    "            if segment_duration != 'overall':\n",
    "                df_pre = df_pre.loc[df_pre[gc.columns.SEGMENT_CAT]==segment_duration]\n",
    "                df_post = df_post.loc[df_post[gc.columns.SEGMENT_CAT]==segment_duration]\n",
    "\n",
    "            range_of_motion_pre_vals = df_pre[measure].values\n",
    "            range_of_motion_post_vals = df_post[measure].values\n",
    "            \n",
    "            if len(range_of_motion_pre_vals) != 0 and len(range_of_motion_post_vals) != 0:\n",
    "                d_effect_size[dataset][segment_duration] = {}\n",
    "                \n",
    "                # point estimate (median) of pre-med and post-med for the true sample\n",
    "                if stat == 'median':\n",
    "                    d_effect_size[dataset][segment_duration]['mu_pre'] = np.median(range_of_motion_pre_vals)\n",
    "                    d_effect_size[dataset][segment_duration]['mu_post'] = np.median(range_of_motion_post_vals)\n",
    "                elif stat == '95':\n",
    "                    d_effect_size[dataset][segment_duration]['mu_pre'] = np.percentile(range_of_motion_pre_vals, 95)\n",
    "                    d_effect_size[dataset][segment_duration]['mu_post'] = np.percentile(range_of_motion_post_vals, 95)\n",
    "\n",
    "                # boostrapping\n",
    "                bootstrap_pre = np.random.choice(range_of_motion_pre_vals, size=(5000, len(range_of_motion_pre_vals)), replace=True)\n",
    "                bootstrap_post = np.random.choice(range_of_motion_post_vals, size=(5000, len(range_of_motion_post_vals)), replace=True)\n",
    "\n",
    "                # point estimate using bootstrapping samples\n",
    "                if stat == 'median': \n",
    "                    bootstrap_samples_pre = np.median(bootstrap_pre, axis=1)\n",
    "                    bootstrap_samples_post = np.median(bootstrap_post, axis=1)\n",
    "                elif stat == '95':\n",
    "                    bootstrap_samples_pre = np.percentile(bootstrap_pre, 95, axis=1)\n",
    "                    bootstrap_samples_post = np.percentile(bootstrap_post, 95, axis=1)\n",
    "                    \n",
    "                # compute difference for std\n",
    "                bootstrap_samples_diff = bootstrap_samples_post - bootstrap_samples_pre\n",
    "\n",
    "                # compute the std\n",
    "                std_bootstrap = np.std(bootstrap_samples_diff)\n",
    "                d_effect_size[dataset][segment_duration]['std'] = std_bootstrap\n",
    "\n",
    "                if segment_duration == 'overall':\n",
    "                    d_diffs[dataset] = bootstrap_samples_diff\n",
    "\n",
    "    return d_effect_size, d_diffs\n",
    "\n",
    "l_diffs = []\n",
    "d_quant = {}\n",
    "for subject in gc.participant_ids.L_PD_IDS:\n",
    "    print(f\"Processing {subject}...\")\n",
    "    d_quantification = {}\n",
    "    \n",
    "    # load gait predictions for free living label\n",
    "    # df_gait_predictions = pd.read_pickle(os.path.join(PDH_PATH_GAIT_PREDICTIONS, gd_model_selected, f'{subject}.pkl'))\n",
    "    df_gait_predictions = pd.read_pickle(os.path.join(r'C:\\Users\\erik_\\Documents\\PhD\\data\\pdh_public\\preprocessed_data\\2.gait_predictions', gd_model_selected, f'{subject}.pkl'))\n",
    "    df_gait_predictions = df_gait_predictions.loc[df_gait_predictions[gc.columns.SIDE]==gc.descriptives.MOST_AFFECTED_SIDE]\n",
    "\n",
    "    if subject in gc.participant_ids.L_HC_IDS:\n",
    "        df_gait_predictions[gc.columns.PRE_OR_POST] = gc.descriptives.CONTROLS\n",
    "\n",
    "    df_gait_predictions = df_gait_predictions.loc[df_gait_predictions[gc.columns.FREE_LIVING_LABEL]=='Walking', [gc.columns.TIME, gc.columns.PRE_OR_POST]]\n",
    "\n",
    "    df_gait_predictions[gc.columns.SEGMENT_NR] = create_segments(df=df_gait_predictions, time_column_name=gc.columns.TIME, gap_threshold_s=gc.parameters.SEGMENT_GAP_GAIT)\n",
    "\n",
    "    # arm activity features\n",
    "    df_features = pd.read_pickle(os.path.join(r'C:\\Users\\erik_\\Documents\\PhD\\data\\pdh_public\\preprocessed_data\\3.arm_activity_features', f'{subject}_{gc.descriptives.MOST_AFFECTED_SIDE}.pkl'))\n",
    "\n",
    "    if subject in gc.participant_ids.L_HC_IDS:\n",
    "        df_features[gc.columns.PRE_OR_POST] = gc.descriptives.CONTROLS\n",
    "\n",
    "    df_features['peak_velocity'] = (df_features['forward_peak_ang_vel_mean'] + df_features['backward_peak_ang_vel_mean'])/2\n",
    "    \n",
    "    df_ts = pd.read_pickle(os.path.join(r'C:\\Users\\erik_\\Documents\\PhD\\data\\pdh_public\\preprocessed_data\\3.arm_activity_features', f'{subject}_{gc.descriptives.MOST_AFFECTED_SIDE}_ts.pkl'))\n",
    "\n",
    "    if subject in gc.participant_ids.L_PD_IDS:\n",
    "        df_ts_exploded = df_ts.explode([gc.columns.TIME, gc.columns.ARM_LABEL])\n",
    "    else:\n",
    "        df_ts_exploded = df_ts.explode([gc.columns.TIME])\n",
    "\n",
    "    if subject in gc.participant_ids.L_HC_IDS:\n",
    "        df_ts_exploded[gc.columns.PRE_OR_POST] = gc.descriptives.CONTROLS\n",
    "\n",
    "    df_features = df_features.drop(columns=[gc.columns.TIME])\n",
    "\n",
    "    df_features = pd.merge(left=df_features, right=df_ts_exploded, how='right', on=[gc.columns.PRE_OR_POST, gc.columns.SEGMENT_NR, gc.columns.WINDOW_NR])\n",
    "    df_features = df_features.groupby([gc.columns.TIME, gc.columns.PRE_OR_POST])[['peak_velocity', 'range_of_motion']].mean().reset_index()\n",
    "\n",
    "    # arm activity predictions\n",
    "    df_predictions = pd.read_pickle(os.path.join(r'C:\\Users\\erik_\\Documents\\PhD\\data\\pdh_public\\preprocessed_data\\4.arm_activity_predictions', asd_model_selected, f'{subject}.pkl'))\n",
    "\n",
    "    # set pred rounded\n",
    "    df_predictions.loc[df_predictions[gc.columns.PRED_OTHER_ARM_ACTIVITY_PROBA]>=0.5, gc.columns.PRED_OTHER_ARM_ACTIVITY] = 1\n",
    "    df_predictions.loc[df_predictions[gc.columns.PRED_OTHER_ARM_ACTIVITY_PROBA]<0.5, gc.columns.PRED_OTHER_ARM_ACTIVITY] = 0\n",
    "\n",
    "    if subject in gc.participant_ids.L_HC_IDS:\n",
    "        df_predictions[gc.columns.PRE_OR_POST] = gc.descriptives.CONTROLS\n",
    "    else:\n",
    "        # boolean for arm activity\n",
    "        df_predictions.loc[df_predictions[gc.columns.ARM_LABEL]=='Gait without other behaviours or other positions', 'other_arm_activity_boolean'] = 0\n",
    "        df_predictions.loc[df_predictions[gc.columns.ARM_LABEL]!='Gait without other behaviours or other positions', 'other_arm_activity_boolean'] = 1\n",
    "        df_predictions.loc[df_predictions[gc.columns.ARM_LABEL]=='Holding an object behind ', gc.columns.ARM_LABEL] = 'Holding an object behind'\n",
    "        df_predictions[gc.columns.ARM_LABEL] = df_predictions.loc[~df_predictions[gc.columns.ARM_LABEL].isna(), gc.columns.ARM_LABEL].apply(lambda x: mp.arm_labels_rename[x] if x in mp.arm_labels_rename.keys() else x)\n",
    "\n",
    "    df = pd.merge(left=df_predictions, right=df_gait_predictions, how='left', on=[gc.columns.TIME, gc.columns.PRE_OR_POST])\n",
    "    df = pd.merge(left=df, right=df_features, how='left', on=[gc.columns.TIME, gc.columns.PRE_OR_POST])\n",
    "\n",
    "    # PROCESS DATA    \n",
    "\n",
    "    # unfiltered gait\n",
    "    d_quantification['unfiltered_gait'] = compute_aggregations(df, l_as_measures)\n",
    "\n",
    "    # filtered gait\n",
    "    d_quantification['filtered_gait'] = compute_aggregations(df.loc[df[gc.columns.PRED_OTHER_ARM_ACTIVITY]==0], l_as_measures)\n",
    "\n",
    "    # no other arm activity (annotated)\n",
    "    if subject in gc.participant_ids.L_PD_IDS:\n",
    "        df_diff = pd.DataFrame()\n",
    "        d_quantification['true_no_other_arm_activity'] = compute_aggregations(df.loc[df['other_arm_activity_boolean']==0], l_as_measures)\n",
    "\n",
    "        es_mrom, diff_mrom = compute_effect_size(df, 'range_of_motion', 'median')\n",
    "        es_prom, diff_prom = compute_effect_size(df, 'range_of_motion', '95')\n",
    "\n",
    "        d_quantification['effect_size'] = {\n",
    "            'median_rom': es_mrom,\n",
    "            '95p_rom': es_prom\n",
    "        }\n",
    "\n",
    "        for dataset in diff_mrom.keys():\n",
    "            df_diff = pd.concat([df_diff, pd.DataFrame([subject, dataset, diff_mrom[dataset], diff_prom[dataset]]).T])\n",
    "\n",
    "        df_diff.columns = [gc.columns.ID, 'dataset', 'median_rom', '95p_rom']\n",
    "\n",
    "        l_diffs.append(df_diff)\n",
    "\n",
    "        d_quant[subject] = d_quantification\n",
    "\n",
    "df_diffs = pd.concat(l_diffs).reset_index(drop=True)\n",
    "\n",
    "path_output = gc.paths.PATH_OUTPUT\n",
    "\n",
    "with open(os.path.join(path_output, 'd_quantification.json'), 'w') as f:\n",
    "    json.dump(d_quant, f)\n",
    "\n",
    "df_diffs.to_pickle(os.path.join(path_output, 'df_effect_size.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pre_or_post</th>\n",
       "      <th>segment_nr</th>\n",
       "      <th>window_nr</th>\n",
       "      <th>other_arm_activity_majority_voting</th>\n",
       "      <th>arm_activity_majority_voting</th>\n",
       "      <th>angle_perc_power</th>\n",
       "      <th>range_of_motion</th>\n",
       "      <th>forward_peak_ang_vel_mean</th>\n",
       "      <th>forward_peak_ang_vel_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cc_3_gyroscope</th>\n",
       "      <th>cc_4_gyroscope</th>\n",
       "      <th>cc_5_gyroscope</th>\n",
       "      <th>cc_6_gyroscope</th>\n",
       "      <th>cc_7_gyroscope</th>\n",
       "      <th>cc_8_gyroscope</th>\n",
       "      <th>cc_9_gyroscope</th>\n",
       "      <th>cc_10_gyroscope</th>\n",
       "      <th>cc_11_gyroscope</th>\n",
       "      <th>cc_12_gyroscope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2869.81</td>\n",
       "      <td>pre</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>non_gait</td>\n",
       "      <td>0.960350</td>\n",
       "      <td>2.790680</td>\n",
       "      <td>4.471501</td>\n",
       "      <td>1.108613</td>\n",
       "      <td>...</td>\n",
       "      <td>18.588606</td>\n",
       "      <td>5.162611</td>\n",
       "      <td>2.236111</td>\n",
       "      <td>2.894101</td>\n",
       "      <td>4.267747</td>\n",
       "      <td>2.038696</td>\n",
       "      <td>-1.331742</td>\n",
       "      <td>2.425525</td>\n",
       "      <td>0.842705</td>\n",
       "      <td>2.557193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2910.81</td>\n",
       "      <td>pre</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>non_gait</td>\n",
       "      <td>0.996568</td>\n",
       "      <td>30.180450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.436829</td>\n",
       "      <td>6.094022</td>\n",
       "      <td>4.364761</td>\n",
       "      <td>1.692476</td>\n",
       "      <td>5.743548</td>\n",
       "      <td>5.014244</td>\n",
       "      <td>4.923274</td>\n",
       "      <td>4.430027</td>\n",
       "      <td>1.104618</td>\n",
       "      <td>1.824498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2911.56</td>\n",
       "      <td>pre</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>non_gait</td>\n",
       "      <td>0.998259</td>\n",
       "      <td>10.916390</td>\n",
       "      <td>61.101962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.453151</td>\n",
       "      <td>8.591188</td>\n",
       "      <td>5.783162</td>\n",
       "      <td>3.657639</td>\n",
       "      <td>-0.987363</td>\n",
       "      <td>4.450801</td>\n",
       "      <td>2.118090</td>\n",
       "      <td>1.922192</td>\n",
       "      <td>2.092054</td>\n",
       "      <td>2.585885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2912.31</td>\n",
       "      <td>pre</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>non_gait</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>16.340827</td>\n",
       "      <td>63.032681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.336731</td>\n",
       "      <td>9.190250</td>\n",
       "      <td>3.660480</td>\n",
       "      <td>2.771963</td>\n",
       "      <td>0.171681</td>\n",
       "      <td>4.351659</td>\n",
       "      <td>1.354682</td>\n",
       "      <td>0.434069</td>\n",
       "      <td>2.887293</td>\n",
       "      <td>2.640319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2913.06</td>\n",
       "      <td>pre</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>non_gait</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>16.655821</td>\n",
       "      <td>63.032681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.256392</td>\n",
       "      <td>9.554277</td>\n",
       "      <td>5.077694</td>\n",
       "      <td>6.286526</td>\n",
       "      <td>1.801317</td>\n",
       "      <td>2.816584</td>\n",
       "      <td>0.891632</td>\n",
       "      <td>1.728194</td>\n",
       "      <td>3.181987</td>\n",
       "      <td>3.919181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time pre_or_post  segment_nr  window_nr  \\\n",
       "0  2869.81         pre           1          1   \n",
       "1  2910.81         pre           2          1   \n",
       "2  2911.56         pre           2          2   \n",
       "3  2912.31         pre           2          3   \n",
       "4  2913.06         pre           2          4   \n",
       "\n",
       "   other_arm_activity_majority_voting arm_activity_majority_voting  \\\n",
       "0                                True                     non_gait   \n",
       "1                                True                     non_gait   \n",
       "2                                True                     non_gait   \n",
       "3                                True                     non_gait   \n",
       "4                                True                     non_gait   \n",
       "\n",
       "   angle_perc_power  range_of_motion  forward_peak_ang_vel_mean  \\\n",
       "0          0.960350         2.790680                   4.471501   \n",
       "1          0.996568        30.180450                   0.000000   \n",
       "2          0.998259        10.916390                  61.101962   \n",
       "3          0.998553        16.340827                  63.032681   \n",
       "4          0.999479        16.655821                  63.032681   \n",
       "\n",
       "   forward_peak_ang_vel_std  ...  cc_3_gyroscope  cc_4_gyroscope  \\\n",
       "0                  1.108613  ...       18.588606        5.162611   \n",
       "1                  0.000000  ...       18.436829        6.094022   \n",
       "2                  0.000000  ...       24.453151        8.591188   \n",
       "3                  0.000000  ...       24.336731        9.190250   \n",
       "4                  0.000000  ...       20.256392        9.554277   \n",
       "\n",
       "   cc_5_gyroscope  cc_6_gyroscope  cc_7_gyroscope  cc_8_gyroscope  \\\n",
       "0        2.236111        2.894101        4.267747        2.038696   \n",
       "1        4.364761        1.692476        5.743548        5.014244   \n",
       "2        5.783162        3.657639       -0.987363        4.450801   \n",
       "3        3.660480        2.771963        0.171681        4.351659   \n",
       "4        5.077694        6.286526        1.801317        2.816584   \n",
       "\n",
       "   cc_9_gyroscope  cc_10_gyroscope  cc_11_gyroscope  cc_12_gyroscope  \n",
       "0       -1.331742         2.425525         0.842705         2.557193  \n",
       "1        4.923274         4.430027         1.104618         1.824498  \n",
       "2        2.118090         1.922192         2.092054         2.585885  \n",
       "3        1.354682         0.434069         2.887293         2.640319  \n",
       "4        0.891632         1.728194         3.181987         3.919181  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.read_pickle(os.path.join(r'C:\\Users\\erik_\\Documents\\PhD\\data\\pdh_public\\preprocessed_data\\3.arm_activity_features', f'{subject}_{gc.descriptives.MOST_AFFECTED_SIDE}.pkl'))\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-at-home-4UNzdMX4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
