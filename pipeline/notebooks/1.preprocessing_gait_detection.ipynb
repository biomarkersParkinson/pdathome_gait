{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing gait detection\n",
    "This script preprocesses the sensor data, tabulates it into windows, and generates features. \n",
    "\n",
    "Execution time $\\approx$ 50s per participant (amounting to 35 minutes in total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdathome.constants import global_constants as gc\n",
    "# from pdathome.preprocessing import preprocess_gait_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hbv002...\n",
      "Processing hbv012...\n",
      "Processing hbv014...\n",
      "Processing hbv015...\n",
      "Processing hbv016...\n",
      "Processing hbv017...\n",
      "Processing hbv022...\n",
      "Processing hbv024...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 138\u001b[0m\n\u001b[0;32m    134\u001b[0m df_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_features, df_temporal_features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# transform the signals from the temporal domain to the spectral domain using the fast fourier transform\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# and extract spectral features\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m df_spectral_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_spectral_domain_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43msensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindowed_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccel_windowed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m df_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_features, df_spectral_features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    146\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gc\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mPATH_GAIT_FEATURES, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\erik_\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pd-at-home-4UNzdMX4-py3.11\\Lib\\site-packages\\paradigma\\gait\\feature_extraction.py:749\u001b[0m, in \u001b[0;36mextract_spectral_domain_features\u001b[1;34m(config, sensor, windowed_data)\u001b[0m\n\u001b[0;32m    746\u001b[0m     feature_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dominant_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m freq\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# Compute total power in the PSD\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m total_power_psd \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_total_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpsd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# Compute MFCCs\u001b[39;00m\n\u001b[0;32m    752\u001b[0m mfccs \u001b[38;5;241m=\u001b[39m compute_mfccs(\n\u001b[0;32m    753\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    754\u001b[0m     total_power_array\u001b[38;5;241m=\u001b[39mtotal_power_psd,\n\u001b[0;32m    755\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\erik_\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pd-at-home-4UNzdMX4-py3.11\\Lib\\site-packages\\paradigma\\gait\\feature_extraction.py:145\u001b[0m, in \u001b[0;36mcompute_total_power\u001b[1;34m(psd)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_total_power\u001b[39m(psd: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Compute the total power by summing the power spectral density (PSD) across frequency bins.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m        window and each sensor axis.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erik_\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pd-at-home-4UNzdMX4-py3.11\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erik_\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pd-at-home-4UNzdMX4-py3.11\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from paradigma.config import IMUConfig, GaitFeatureExtractionConfig\n",
    "from paradigma.pipelines.gait_pipeline import extract_temporal_domain_features, extract_spectral_domain_features\n",
    "from paradigma.preprocessing import butterworth_filter\n",
    "from paradigma.segmenting import tabulate_windows\n",
    "\n",
    "def compute_mode(data):\n",
    "    \"\"\"Computes the mode for 1D data using np.unique.\"\"\"\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    max_count_index = np.argmax(counts)\n",
    "    return values[max_count_index], counts[max_count_index]\n",
    "\n",
    "def is_majority(data, target=\"Walking\"):\n",
    "    \"\"\"Checks if 'target' occurs more than half the time in 1D data.\"\"\"\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    target_count = counts[values == target].sum() if target in values else 0\n",
    "    return target_count > (len(data) / 2)\n",
    "\n",
    "for subject in gc.participant_ids.PD_IDS + gc.participant_ids.HC_IDS:\n",
    "    print(f'Processing {subject}...')\n",
    "    for side in [gc.descriptives.MOST_AFFECTED_SIDE, gc.descriptives.LEAST_AFFECTED_SIDE]:\n",
    "        df = pd.read_parquet(os.path.join(gc.paths.PATH_PREPARED_DATA, f'{subject}_{side}.parquet'))\n",
    "\n",
    "        config = IMUConfig()\n",
    "        config.acceleration_units = 'g'\n",
    "\n",
    "        # Change to correct units [g]\n",
    "        df[gc.columns.ACCELEROMETER_COLS] = df[gc.columns.ACCELEROMETER_COLS] / 9.81 if config.acceleration_units == 'm/s^2' else df[gc.columns.ACCELEROMETER_COLS]\n",
    "\n",
    "        # Extract accelerometer data\n",
    "        accel_data = df[gc.columns.ACCELEROMETER_COLS].values\n",
    "\n",
    "        filter_configs = {\n",
    "            \"hp\": {\"result_columns\": gc.columns.ACCELEROMETER_COLS, \"replace_original\": True},\n",
    "            \"lp\": {\"result_columns\": [f'{col}_grav' for col in gc.columns.ACCELEROMETER_COLS], \"replace_original\": False},\n",
    "        }\n",
    "\n",
    "        # Apply filters in a loop\n",
    "        for passband, filter_config in filter_configs.items():\n",
    "            filtered_data = butterworth_filter(\n",
    "                data=accel_data,\n",
    "                order=config.filter_order,\n",
    "                cutoff_frequency=config.lower_cutoff_frequency,\n",
    "                passband=passband,\n",
    "                sampling_frequency=config.sampling_frequency,\n",
    "            )\n",
    "\n",
    "            # Replace or add new columns based on configuration\n",
    "            df[filter_config[\"result_columns\"]] = filtered_data\n",
    "\n",
    "        config = GaitFeatureExtractionConfig()\n",
    "\n",
    "        windowed_data = []\n",
    "\n",
    "        l_windowed_cols = [\n",
    "            gc.columns.TIME, gc.columns.FREE_LIVING_LABEL\n",
    "            ] + config.accelerometer_cols + config.gravity_cols\n",
    "        \n",
    "        if subject in gc.participant_ids.PD_IDS:\n",
    "            l_windowed_cols += [gc.columns.ARM_LABEL]\n",
    "\n",
    "            df_grouped = df.groupby(gc.columns.PRE_OR_POST, sort=False)\n",
    "            order = ['pre', 'post']\n",
    "\n",
    "            for label in order:\n",
    "                if label in df_grouped.groups:  # Ensure the label exists in the groups\n",
    "                    group = df_grouped.get_group(label)\n",
    "                    windows = tabulate_windows(\n",
    "                        df=group,\n",
    "                        columns=l_windowed_cols,\n",
    "                        window_length_s=config.window_length_s,\n",
    "                        window_step_length_s=config.window_step_length_s,\n",
    "                        fs=config.sampling_frequency\n",
    "                    )\n",
    "                    if len(windows) > 0:  # Skip if no windows are created\n",
    "                        windowed_data.append(windows)\n",
    "\n",
    "        else:\n",
    "            windows = tabulate_windows(\n",
    "                df=df,\n",
    "                columns=l_windowed_cols,\n",
    "                window_length_s=config.window_length_s,\n",
    "                window_step_length_s=config.window_step_length_s,\n",
    "                fs=config.sampling_frequency\n",
    "            )\n",
    "            if len(windows) > 0:  # Skip if no windows are created\n",
    "                windowed_data.append(windows)\n",
    "\n",
    "        if len(windowed_data) > 0:\n",
    "            windowed_data = np.concatenate(windowed_data, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"No windows were created from the given data.\")\n",
    "        \n",
    "        df_features = pd.DataFrame()\n",
    "\n",
    "        df_features[gc.columns.TIME] = sorted(windowed_data[:, 0, l_windowed_cols.index(gc.columns.TIME)])\n",
    "\n",
    "        if subject in gc.participant_ids.PD_IDS:\n",
    "            df_features = pd.merge(left=df_features, right=df[[gc.columns.TIME, gc.columns.PRE_OR_POST]], how='left', on=gc.columns.TIME) \n",
    "\n",
    "        # Calulate the mode of the labels\n",
    "        windowed_labels = windowed_data[:, :, l_windowed_cols.index(gc.columns.FREE_LIVING_LABEL)]\n",
    "        modes_and_counts = np.apply_along_axis(lambda x: compute_mode(x), axis=1, arr=windowed_labels)\n",
    "        modes, counts = zip(*modes_and_counts)\n",
    "\n",
    "        df_features[gc.columns.ACTIVITY_LABEL_MAJORITY_VOTING] = modes\n",
    "        df_features[gc.columns.GAIT_MAJORITY_VOTING] = [is_majority(window) for window in windowed_labels]\n",
    "\n",
    "        if subject in gc.participant_ids.PD_IDS:\n",
    "            windowed_labels = windowed_data[:, :, l_windowed_cols.index(gc.columns.ARM_LABEL)]\n",
    "            modes_and_counts = np.apply_along_axis(lambda x: compute_mode(x), axis=1, arr=windowed_labels)\n",
    "            modes, counts = zip(*modes_and_counts)\n",
    "\n",
    "            df_features[gc.columns.ARM_LABEL_MAJORITY_VOTING] = modes\n",
    "            df_features[gc.columns.NO_OTHER_ARM_ACTIVITY_MAJORITY_VOTING] = [is_majority(window, target=\"Gait without other behaviours or other positions\") for window in windowed_labels]\n",
    "\n",
    "        # compute statistics of the temporal domain signals\n",
    "        accel_indices = [l_windowed_cols.index(x) for x in config.accelerometer_cols]\n",
    "        grav_indices = [l_windowed_cols.index(x) for x in config.gravity_cols]\n",
    "\n",
    "        accel_windowed = np.asarray(windowed_data[:, :, np.min(accel_indices):np.max(accel_indices) + 1], dtype=float)\n",
    "        grav_windowed = np.asarray(windowed_data[:, :, np.min(grav_indices):np.max(grav_indices) + 1], dtype=float)\n",
    "\n",
    "        df_temporal_features = extract_temporal_domain_features(\n",
    "            config=config,\n",
    "            windowed_acc=accel_windowed,\n",
    "            windowed_grav=grav_windowed,\n",
    "            grav_stats=['mean', 'std']\n",
    "        )\n",
    "\n",
    "        df_features = pd.concat([df_features, df_temporal_features], axis=1)\n",
    "\n",
    "        # transform the signals from the temporal domain to the spectral domain using the fast fourier transform\n",
    "        # and extract spectral features\n",
    "        df_spectral_features = extract_spectral_domain_features(\n",
    "            config=config,\n",
    "            sensor=config.sensor,\n",
    "            windowed_data=accel_windowed,\n",
    "        )\n",
    "\n",
    "        df_features = pd.concat([df_features, df_spectral_features], axis=1)\n",
    "        \n",
    "        file_path = os.path.join(gc.paths.PATH_GAIT_FEATURES, f'{subject}_{side}.parquet')\n",
    "        df_features.to_parquet(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-at-home-4UNzdMX4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
