{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tsdf\n",
    "\n",
    "import datetime\n",
    "import dateutil.parser as parser\n",
    "\n",
    "from paradigma.imu_preprocessing import IMUPreprocessingConfig, resample_data, transform_time_array\n",
    "from paradigma.constants import TimeUnit, DataColumns\n",
    "from paradigma.windowing import create_segments\n",
    "from paradigma.feature_extraction import extract_temporal_domain_features, extract_spectral_domain_features\n",
    "\n",
    "\n",
    "from paradigma.imu_preprocessing import butterworth_filter\n",
    "from paradigma.gait_analysis_config import GaitFeatureExtractionConfig\n",
    "import numpy as np\n",
    "\n",
    "def meta_dict_single_subject(data_path, meta_file_name):\n",
    "    meta_dict = {}\n",
    "    l_segments = glob.glob(os.path.join(data_path, meta_file_name))\n",
    "    meta_dict['segments'] = sorted(l_segments, reverse=False)\n",
    "    meta_dict['file_names'] = []\n",
    "    for name in meta_dict['segments']:\n",
    "        meta_dict['file_names'].append(os.path.basename(name))\n",
    "        meta_dict['n_files'] = len(meta_dict['file_names'])\n",
    "\n",
    "    return meta_dict\n",
    "\n",
    "def load_tsdf_data(path_to_data, meta_filename, values_filename, time_filename):\n",
    "\n",
    "    with open(os.path.join(path_to_data, meta_filename)) as f:       \n",
    "        metadata_dict = tsdf.load_metadata_legacy_file(f)\n",
    "\n",
    "    metadata_values = metadata_dict[values_filename]\n",
    "    metadata_time = metadata_dict[time_filename]\n",
    "\n",
    "    df = tsdf.load_dataframe_from_binaries([metadata_time, metadata_values], tsdf.constants.ConcatenationType.columns)\n",
    "\n",
    "    return df, metadata_values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def tabulate_windows(config, df, agg_func='first'):\n",
    "    \"\"\"\n",
    "    Efficiently creates a windowed dataframe from the input dataframe using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe, where each row represents a timestamp (0.01 sec).\n",
    "        window_size_s (int): The number of seconds per window.\n",
    "        step_size_s (int): The number of seconds to shift between windows.\n",
    "        single_value_cols (list): List of columns where a single value (e.g., mean) is needed.\n",
    "        list_value_cols (list): List of columns where all 600 values should be stored in a list.\n",
    "        agg_func (str or function): Aggregation function for single-value columns (e.g., 'mean', 'first').\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The windowed dataframe.\n",
    "    \"\"\"\n",
    "    # If single_value_cols or list_value_cols is None, default to an empty list\n",
    "    if config.single_value_cols is None:\n",
    "        config.single_value_cols = []\n",
    "    if config.list_value_cols is None:\n",
    "        config.list_value_cols = []\n",
    "\n",
    "    window_length = int(config.window_length_s * config.sampling_frequency)\n",
    "    window_step_size = int(config.window_step_size_s * config.sampling_frequency)\n",
    "\n",
    "    n_rows = len(df)\n",
    "    if window_length > n_rows:\n",
    "        raise ValueError(f\"Window size ({window_length}) cannot be greater than the number of rows ({n_rows}) in the dataframe.\")\n",
    "    \n",
    "    # Create indices for window start positions \n",
    "    window_starts = np.arange(0, n_rows - window_length + 1, window_step_size)\n",
    "    \n",
    "    # Prepare the result for the final DataFrame\n",
    "    result = []\n",
    "    \n",
    "    # Handle single value columns with vectorized operations\n",
    "    agg_func_map = {\n",
    "        'mean': np.mean,\n",
    "        'first': lambda x: x[0],\n",
    "    }\n",
    "\n",
    "    # Check if agg_func is a callable (custom function) or get the function from the map\n",
    "    if callable(agg_func):\n",
    "        agg_func_np = agg_func\n",
    "    else:\n",
    "        agg_func_np = agg_func_map.get(agg_func, agg_func_map['mean'])  # Default to 'mean' if agg_func is not recognized\n",
    "\n",
    "        \n",
    "    for window_nr, start in enumerate(window_starts, 1):\n",
    "        end = start + window_length\n",
    "        window = df.iloc[start:end]\n",
    "\n",
    "        agg_data = {\n",
    "            'window_nr': window_nr,\n",
    "            'window_start': window[config.time_colname].iloc[0],\n",
    "            'window_end': window[config.time_colname].iloc[-1],\n",
    "        }\n",
    "        \n",
    "        # Aggregate single-value columns\n",
    "        for col in config.single_value_cols:\n",
    "            if col in window.columns:  # Only process columns that exist in the window\n",
    "                agg_data[col] = agg_func_np(window[col].values)\n",
    "        \n",
    "        # Collect list-value columns efficiently using numpy slicing\n",
    "        for col in config.list_value_cols:\n",
    "            if col in window.columns:  # Only process columns that exist in the window\n",
    "                agg_data[col] = window[col].values.tolist()\n",
    "\n",
    "        result.append(agg_data)\n",
    "    \n",
    "    # Convert result list into a DataFrame\n",
    "    windowed_df = pd.DataFrame(result)\n",
    "    \n",
    "    # Ensure the column order is as desired: window_nr, window_start, window_end, pre_or_post, and then the rest\n",
    "    desired_order = ['window_nr', 'window_start', 'window_end'] + config.single_value_cols + config.list_value_cols\n",
    "    \n",
    "    return windowed_df[desired_order]\n",
    "\n",
    "def store_tsdf(df, d_cols_time, d_cols_features, metadata_general, path_output):\n",
    "\n",
    "    if not os.path.exists(path_output):\n",
    "        os.makedirs(path_output)\n",
    "\n",
    "    try:\n",
    "        metadata_time = metadata_general.copy()\n",
    "    except AttributeError:\n",
    "        metadata_time = metadata_general.get_plain_tsdf_dict_copy()\n",
    "    metadata_time['channels'] = list(d_cols_time.keys())\n",
    "    metadata_time['units'] = list(d_cols_time.values())\n",
    "    metadata_time['data_type'] = int\n",
    "\n",
    "    try:\n",
    "        metadata_features = metadata_general.copy()\n",
    "    except AttributeError:\n",
    "        metadata_features = metadata_general.get_plain_tsdf_dict_copy()\n",
    "    metadata_features['channels'] = list(d_cols_features.keys())\n",
    "    metadata_features['units'] = list(d_cols_features.values())\n",
    "    metadata_features['data_type'] = float\n",
    "\n",
    "    # create numpy\n",
    "    data_time = df[d_cols_time.keys()].to_numpy()\n",
    "    data_features = df[d_cols_features.keys()].to_numpy()\n",
    "\n",
    "    # write binary\n",
    "    l_metafiles = []\n",
    "    l_metafiles.append(tsdf.write_binary_file(path_output, f\"values.bin\", data_features, metadata_features))\n",
    "    l_metafiles.append(tsdf.write_binary_file(path_output, f\"time.bin\", data_time, metadata_time))\n",
    "\n",
    "    # store metadata\n",
    "    tsdf.write_metadata(l_metafiles, os.path.join(path_output, f\"meta.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:54:17.366028: Preprocessing gait...\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:55:38.413769: Split segment 1.1 Features extracted.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:55:38.430137: Split segment 1.1 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:56:28.747732: Split segment 1.2 Features extracted.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:56:28.761026: Split segment 1.2 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:56:28.761026: Segment 1 # split segments to concatenate: 2.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:56:28.778079: Segment 1 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:57:59.931424: Split segment 2.1 Features extracted.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:57:59.962653: Split segment 2.1 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:59:19.933473: Split segment 2.2 Features extracted.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 14:59:19.960057: Split segment 2.2 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.177928: Split segment 2.3 Features extracted.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.194710: Split segment 2.3 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.194710: Segment 2 # split segments to concatenate: 3.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.261032: Segment 2 finished.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.261032: # segments to concatenate: 2\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.545935: Storing tsdf.\n",
      "Week 1 - ID POMU0053EE359AA64252 - Datetime 2024-10-17 15:00:18.545935: Gait preprocessed.\n"
     ]
    }
   ],
   "source": [
    "subject = 'POMU0053EE359AA64252'\n",
    "week_nr = '1'\n",
    "\n",
    "path_sensor_data = os.path.join(r'C:\\Users\\erik_\\Documents\\PhD\\data\\ppp\\raw', week_nr, subject)\n",
    "meta_filename_raw = f'WatchData.IMU.Week{week_nr}.raw_segment*_meta.json'\n",
    "\n",
    "meta_dict = meta_dict_single_subject(data_path=path_sensor_data, meta_file_name=meta_filename_raw)\n",
    "try:\n",
    "    n_files = meta_dict['n_files']\n",
    "except KeyError:\n",
    "    print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Could not determine n_files while preprocessing gait. Continuing with next participant.\")\n",
    "else:\n",
    "    print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Preprocessing gait...\")\n",
    "\n",
    "    l_segment_dfs = []\n",
    "\n",
    "    n_segments_completed = 0\n",
    "    for segment_idx in range(1, n_files + 1):\n",
    "        meta_fullpath = os.path.join(path_sensor_data, meta_dict['file_names'][segment_idx - 1])\n",
    "        segment_nr = meta_dict['file_names'][segment_idx - 1].split('_')[1][-4:]\n",
    "\n",
    "        meta_filename_raw = f'WatchData.IMU.Week{week_nr}.raw_segment{segment_nr}_meta.json'\n",
    "        values_filename_raw = meta_filename_raw.replace('_meta.json', '_samples.bin')\n",
    "        time_filename_raw = meta_filename_raw.replace('_meta.json', '_time.bin')\n",
    "\n",
    "        df_ts, metadata_ts = load_tsdf_data(path_sensor_data, meta_filename_raw, values_filename_raw, time_filename_raw) \n",
    "\n",
    "        if n_segments_completed == 0:\n",
    "            start_iso_first_segment = parser.parse(metadata_ts.start_iso8601)\n",
    "            start_iso_this_segment = parser.parse(metadata_ts.start_iso8601)\n",
    "        else:\n",
    "            start_iso_this_segment = parser.parse(metadata_ts.start_iso8601)\n",
    "\n",
    "            gap_to_first_segment_start_ms = (start_iso_this_segment - start_iso_first_segment).total_seconds() * 1000\n",
    "\n",
    "        end_iso_this_segment = parser.parse(metadata_ts.end_iso8601)\n",
    "\n",
    "        config = IMUPreprocessingConfig()\n",
    "\n",
    "        # TODO: Once Paradigma is updated with new axes inversion rules, swap to preprocess_imu_data\n",
    "        # FROM HERE\n",
    "        # Rename columns\n",
    "        df_ts = df_ts.rename(columns={f'rotation_{a}': f'gyroscope_{a}' for a in ['x', 'y', 'z']})\n",
    "        df_ts = df_ts.rename(columns={f'acceleration_{a}': f'accelerometer_{a}' for a in ['x', 'y', 'z']})\n",
    "\n",
    "        # Convert to relative seconds from delta milliseconds\n",
    "        df_ts[config.time_colname] = transform_time_array(\n",
    "            time_array=df_ts[config.time_colname],\n",
    "            scale_factor=1000, \n",
    "            input_unit_type = TimeUnit.DIFFERENCE_MS,\n",
    "            output_unit_type = TimeUnit.RELATIVE_MS)\n",
    "        \n",
    "        # Decimate and interpolate data\n",
    "        df_ts = resample_data(\n",
    "            df=df_ts,\n",
    "            time_column=config.time_colname,\n",
    "            time_unit_type=TimeUnit.RELATIVE_MS,\n",
    "            unscaled_column_names = list(config.d_channels_imu.keys()),\n",
    "            scale_factors=metadata_ts.scale_factors,\n",
    "            resampling_frequency=config.sampling_frequency)\n",
    "        \n",
    "        # Invert axes to adhere to pdathome standards\n",
    "        if config.side_watch == 'right':\n",
    "            df_ts[DataColumns.ACCELEROMETER_X] *= -1\n",
    "            df_ts[DataColumns.GYROSCOPE_Y] *= -1\n",
    "            df_ts[DataColumns.GYROSCOPE_Z] *= -1\n",
    "\n",
    "        split_segment_len = 3000000\n",
    "\n",
    "        # Filter accelerometer data\n",
    "        for col in config.d_channels_accelerometer.keys():\n",
    "\n",
    "            # Change to correct units [g]\n",
    "            if config.acceleration_units == 'm/s^2':\n",
    "                df_ts[col] /= 9.81\n",
    "\n",
    "            for result, side_pass in zip(['filt', 'grav'], ['hp', 'lp']):\n",
    "                df_ts[f'{result}_{col}'] = butterworth_filter(\n",
    "                    single_sensor_col=np.array(df_ts[col]),\n",
    "                    order=config.filter_order,\n",
    "                    cutoff_frequency=config.lower_cutoff_frequency,\n",
    "                    passband=side_pass,\n",
    "                    sampling_frequency=config.sampling_frequency,\n",
    "                    )\n",
    "                \n",
    "            df_ts = df_ts.drop(columns=[col])\n",
    "            df_ts = df_ts.rename(columns={f'filt_{col}': col})\n",
    "\n",
    "        config = GaitFeatureExtractionConfig()\n",
    "\n",
    "        n_split_segments = (df_ts.shape[0] // split_segment_len) + 1\n",
    "\n",
    "        l_split_dfs = []\n",
    "        n_split_segments_completed = 0\n",
    "\n",
    "        config.single_value_cols = None\n",
    "        config.list_value_cols = config.l_accelerometer_cols + config.l_gravity_cols\n",
    "\n",
    "        \n",
    "        for split_segment_iter in range(1,n_split_segments+1):\n",
    "            if n_split_segments > 1:\n",
    "                df_split = df_ts.iloc[(split_segment_iter-1)*split_segment_len:(split_segment_iter)*split_segment_len, :].copy()\n",
    "            else:\n",
    "                df_split = df_ts\n",
    "\n",
    "            df_split['segment_nr'] = create_segments(df=df_split, time_column_name='time', gap_threshold_s=1.5)\n",
    "\n",
    "            if n_segments_completed>0 and n_split_segments_completed==0:\n",
    "                df_split['segment_nr'] += max_segment_nr\n",
    "            elif n_split_segments_completed>0:\n",
    "                df_split['segment_nr'] += max_split_segment_nr\n",
    "\n",
    "            max_split_segment_nr = df_split['segment_nr'].max()\n",
    "\n",
    "            l_dfs_segments = []\n",
    "            df_split_copy = df_split.copy()\n",
    "\n",
    "            for segment_nr in df_split_copy['segment_nr'].unique():\n",
    "                df_split = df_split_copy.loc[df_split_copy['segment_nr']==segment_nr].copy().reset_index(drop=True)\n",
    "                df_windows_segment = tabulate_windows(\n",
    "                    df=df_split, config=config,\n",
    "                )\n",
    "                df_windows_segment['segment_nr'] = segment_nr\n",
    "\n",
    "                l_dfs_segments.append(df_windows_segment)\n",
    "\n",
    "            if len(l_dfs_segments) > 1:\n",
    "                df_windows = pd.concat(l_dfs_segments).reset_index(drop=True)\n",
    "            elif len(l_dfs_segments) == 1:\n",
    "                df_windows = l_dfs_segments[0]\n",
    "            else:\n",
    "                print(\"NO\")\n",
    "\n",
    "            del df_split, df_split_copy\n",
    "\n",
    "            if df_windows is None:\n",
    "                print(\"A\")\n",
    "            elif df_windows.shape[0] == 0:\n",
    "                print(\"B\")\n",
    "\n",
    "            df_windows = df_windows.reset_index(drop=True)  \n",
    "\n",
    "            df_windows = extract_temporal_domain_features(config=config, df_windowed=df_windows, l_gravity_stats=['mean', 'std'])\n",
    "\n",
    "            df_windows = extract_spectral_domain_features(config=config, df_windowed=df_windows, sensor='accelerometer', l_sensor_colnames=config.l_accelerometer_cols)\n",
    "\n",
    "            print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Split segment {segment_idx}.{split_segment_iter} Features extracted.\")\n",
    "\n",
    "            for col in df_windows.columns:\n",
    "                if pd.isna(df_windows[col]).any():\n",
    "                    df_windows[col] = df_windows[col].fillna(0)\n",
    "\n",
    "            l_split_dfs.append(df_windows)\n",
    "\n",
    "            n_split_segments_completed += 1\n",
    "\n",
    "            print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Split segment {segment_idx}.{split_segment_iter} finished.\")\n",
    "\n",
    "        print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Segment {segment_idx} # split segments to concatenate: {len(l_split_dfs)}.\")\n",
    "\n",
    "        if len(l_split_dfs) > 1:\n",
    "            df = pd.concat(l_split_dfs)\n",
    "        elif len(l_split_dfs) == 1:\n",
    "            df = l_split_dfs[0]\n",
    "        else:\n",
    "            print(\"C\")\n",
    "\n",
    "        df['time'] *= 1000\n",
    "\n",
    "        if n_segments_completed>0:\n",
    "            df['time'] += gap_to_first_segment_start_ms\n",
    "\n",
    "        max_segment_nr = df['segment_nr'].max()\n",
    "\n",
    "        l_segment_dfs.append(df)\n",
    "    \n",
    "        n_segments_completed += 1\n",
    "\n",
    "        print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Segment {segment_idx} finished.\")\n",
    "\n",
    "    print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: # segments to concatenate: {len(l_segment_dfs)}\")\n",
    "\n",
    "    if len(l_segment_dfs) == 0:\n",
    "        print(\"D\")\n",
    "    elif len(l_segment_dfs) > 1:\n",
    "        df = pd.concat(l_segment_dfs)\n",
    "    else:\n",
    "        df = l_segment_dfs[0]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Storing tsdf.\")\n",
    "\n",
    "    metadata = {\n",
    "        'subject_id': subject,\n",
    "        'study_id': 'PPP',\n",
    "        'device_id': 'Verily',\n",
    "        'endianness': 'little',\n",
    "        'metadata_version': '0.1',\n",
    "        'side_watch': 'left',\n",
    "        'start_iso8601': str(start_iso_first_segment.isoformat()),\n",
    "        'end_iso8601': str(end_iso_this_segment.isoformat()),\n",
    "        'window_size_sec': config.window_length_s,\n",
    "        'step_size_sec': config.window_step_size_s\n",
    "    }\n",
    "\n",
    "    # store_tsdf(df, d_cols_time, d_cols_features_gait, metadata, path_output)\n",
    "\n",
    "    print(f\"Week {week_nr} - ID {subject} - Datetime {datetime.datetime.now()}: Gait preprocessed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pdathome.constants import global_constants as gc\n",
    "from pdathome.classification import train_test_filtering_gait, store_filtering_gait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_classifiers = [gc.classifiers.LOGISTIC_REGRESSION, gc.classifiers.RANDOM_FOREST]\n",
    "gsearch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in gc.participant_ids.L_PD_IDS:\n",
    "    train_test_filtering_gait(subject, gsearch=gsearch, l_classifiers=l_classifiers)\n",
    "\n",
    "store_filtering_gait(l_classifiers=l_classifiers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-at-home-4UNzdMX4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
