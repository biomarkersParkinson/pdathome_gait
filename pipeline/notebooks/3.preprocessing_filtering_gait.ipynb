{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdathome.constants import global_constants as gc\n",
    "from pdathome.preprocessing import preprocess_filtering_gait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 2024-11-29 11:33:05.401901 - hbv002 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:12.937718 - hbv002 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:13.125996 - hbv012 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:21.328181 - hbv012 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:21.528124 - hbv014 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:28.924726 - hbv014 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:29.101190 - hbv015 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:37.111695 - hbv015 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:37.303838 - hbv016 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:44.344427 - hbv016 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:44.519984 - hbv017 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:51.933952 - hbv017 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:52.110311 - hbv022 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:33:59.747715 - hbv022 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:33:59.922711 - hbv024 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:10.031510 - hbv024 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:10.268688 - hbv039 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:18.638384 - hbv039 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:18.828093 - hbv043 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:26.023143 - hbv043 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:26.185028 - hbv047 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:34.084969 - hbv047 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:34.261519 - hbv054 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:39.556361 - hbv054 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:39.691833 - hbv065 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:45.528668 - hbv065 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:45.678201 - hbv077 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:34:54.726116 - hbv077 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:34:54.912721 - hbv079 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:04.337786 - hbv079 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:04.561507 - hbv090 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:16.433380 - hbv090 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:16.683383 - hbv013 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:22.023794 - hbv013 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:22.143962 - hbv018 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:28.617789 - hbv018 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:28.761431 - hbv023 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:41.670630 - hbv023 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:41.959914 - hbv038 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:51.027529 - hbv038 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:51.229455 - hbv058 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:35:58.244951 - hbv058 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:35:58.411724 - hbv063 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:06.200186 - hbv063 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:06.384098 - hbv053 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:10.528881 - hbv053 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:10.617567 - hbv072 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:14.867326 - hbv072 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:14.961886 - hbv073 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:20.395272 - hbv073 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:20.516404 - hbv082 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:26.612254 - hbv082 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:26.758437 - hbv083 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:31.031113 - hbv083 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:31.134838 - hbv084 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:35.546378 - hbv084 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:35.649475 - hbv087 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:41.869968 - hbv087 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:42.001225 - hbv091 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:50.453247 - hbv091 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:50.648454 - hbv093 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:36:57.073599 - hbv093 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:36:57.211740 - hbv097 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:02.236707 - hbv097 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:02.350917 - hbv099 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:07.562019 - hbv099 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:07.692371 - hbv100 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:12.328961 - hbv100 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:12.421352 - hbv106 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:18.445744 - hbv106 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:18.588457 - hbv108 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:22.594423 - hbv108 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:22.678044 - hbv109 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:28.038532 - hbv109 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:28.156175 - hbv110 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:33.281578 - hbv110 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:33.385728 - hbv112 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:39.425929 - hbv112 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:39.550313 - hbv115 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:45.946415 - hbv115 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:46.083787 - hbv117 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:51.684708 - hbv117 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:51.815852 - hbv122 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:37:57.095940 - hbv122 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:37:57.209422 - hbv128 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:38:01.313680 - hbv128 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:38:01.406575 - hbv136 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:38:07.585035 - hbv136 - Finished preprocessing filtering gait.\n",
      "Time 2024-11-29 11:38:07.716275 - hbv081 - Starting preprocessing filtering gait ...\n",
      "Time 2024-11-29 11:38:11.196093 - hbv081 - Finished preprocessing filtering gait.\n"
     ]
    }
   ],
   "source": [
    "for subject in gc.participant_ids.L_PD_IDS + gc.participant_ids.L_HC_IDS:\n",
    "    preprocess_filtering_gait(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "from paradigma.gait.feature_extraction import extract_temporal_domain_features, extract_spectral_domain_features, \\\n",
    "    pca_transform_gyroscope, compute_angle, remove_moving_average_angle, extract_angle_features\n",
    "from paradigma.gait.gait_analysis_config import GaitFeatureExtractionConfig, ArmActivityFeatureExtractionConfig\n",
    "from paradigma.imu_preprocessing import butterworth_filter\n",
    "from paradigma.preprocessing_config import IMUPreprocessingConfig\n",
    "from paradigma.segmenting import tabulate_windows, create_segments, discard_segments, categorize_segments\n",
    "\n",
    "from pdathome.constants import global_constants as gc, mappings as mp\n",
    "from pdathome.load import load_stage_start_end, load_sensor_data, load_video_annotations\n",
    "from pdathome.utils import save_to_pickle\n",
    "\n",
    "\n",
    "def compute_mode(data):\n",
    "    \"\"\"Computes the mode for 1D data using np.unique.\"\"\"\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    max_count_index = np.argmax(counts)\n",
    "    return values[max_count_index], counts[max_count_index]\n",
    "\n",
    "def is_majority(data, target=\"Walking\"):\n",
    "    \"\"\"Checks if 'target' occurs more than half the time in 1D data.\"\"\"\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    target_count = counts[values == target].sum() if target in values else 0\n",
    "    return target_count > (len(data) / 2)\n",
    "\n",
    "for side in [gc.descriptives.MOST_AFFECTED_SIDE, gc.descriptives.LEAST_AFFECTED_SIDE]:\n",
    "\n",
    "    # load timestamps\n",
    "    df_ts = pd.read_pickle(os.path.join(gc.paths.PATH_PREPARED_DATA, f'{subject}_{side}.pkl'))\n",
    "    df_ts['time'] = df_ts['time'].round(2)\n",
    "\n",
    "    # load gait features\n",
    "    df_features = pd.read_pickle(os.path.join(gc.paths.PATH_GAIT_FEATURES, f'{subject}_{side}.pkl'))\n",
    "\n",
    "    # Load gait predictions\n",
    "    df_pred = pd.read_pickle(os.path.join(gc.paths.PATH_GAIT_PREDICTIONS, gc.classifiers.GAIT_DETECTION_CLASSIFIER_SELECTED, f'{subject}_{side}.pkl'))\n",
    "\n",
    "    # Load classification threshold\n",
    "    with open(os.path.join(gc.paths.PATH_THRESHOLDS, 'gait', f'{gc.classifiers.GAIT_DETECTION_CLASSIFIER_SELECTED}.txt'), 'r') as f:\n",
    "        threshold = float(f.read())\n",
    "\n",
    "    # Determine gait prediction per timestamp\n",
    "    l_cols_features = ['time']\n",
    "    df_predictions = pd.concat([df_features[l_cols_features], df_pred], axis=1)\n",
    "    \n",
    "    imu_config = IMUPreprocessingConfig()\n",
    "    gait_config = GaitFeatureExtractionConfig()\n",
    "    arm_activity_config = ArmActivityFeatureExtractionConfig()\n",
    "\n",
    "    # Step 1: Expand each window into individual timestamps\n",
    "    expanded_data = []\n",
    "    for _, row in df_predictions.iterrows():\n",
    "        start_time = row['time']\n",
    "        proba = row['pred_gait_proba']\n",
    "        timestamps = np.arange(start_time, start_time + gait_config.window_length_s, 1/gc.parameters.DOWNSAMPLED_FREQUENCY)\n",
    "        expanded_data.extend(zip(timestamps, [proba] * len(timestamps)))\n",
    "\n",
    "    # Create a new DataFrame with expanded timestamps\n",
    "    expanded_df = pd.DataFrame(expanded_data, columns=['time', 'pred_gait_proba'])\n",
    "\n",
    "    # Step 2: Round timestamps to avoid floating-point inaccuracies\n",
    "    expanded_df['time'] = expanded_df['time'].round(2)\n",
    "\n",
    "    # Step 3: Aggregate by unique timestamps and calculate the mean probability\n",
    "    expanded_df = expanded_df.groupby('time', as_index=False)['pred_gait_proba'].mean()\n",
    "\n",
    "    df_ts = pd.merge(left=df_ts, right=expanded_df, how='left', on='time')\n",
    "\n",
    "    imu_config.acceleration_units = 'g'\n",
    "    arm_activity_config.list_value_cols += [gc.columns.TIME, gc.columns.FREE_LIVING_LABEL]\n",
    "\n",
    "    # Extract relevant gc.columns for accelerometer data\n",
    "    accel_cols = imu_config.l_accelerometer_cols\n",
    "\n",
    "    # Change to correct units [g]\n",
    "    df_ts[accel_cols] = df_ts[accel_cols] / 9.81 if imu_config.acceleration_units == 'm/s^2' else df_ts[accel_cols]\n",
    "\n",
    "    # Extract accelerometer data\n",
    "    accel_data = df_ts[imu_config.l_accelerometer_cols].values\n",
    "\n",
    "    filter_configs = {\n",
    "        \"hp\": {\"result_columns\": imu_config.l_accelerometer_cols, \"replace_original\": True},\n",
    "        \"lp\": {\"result_columns\": [f'{col}_grav' for col in imu_config.l_accelerometer_cols], \"replace_original\": False},\n",
    "    }\n",
    "\n",
    "    # Apply filters in a loop\n",
    "    for passband, filter_config in filter_configs.items():\n",
    "        filtered_data = butterworth_filter(\n",
    "            data=accel_data,\n",
    "            order=imu_config.filter_order,\n",
    "            cutoff_frequency=imu_config.lower_cutoff_frequency,\n",
    "            passband=passband,\n",
    "            sampling_frequency=imu_config.sampling_frequency,\n",
    "        )\n",
    "\n",
    "        # Replace or add new columns based on configuration\n",
    "        df_ts[filter_config[\"result_columns\"]] = filtered_data\n",
    "\n",
    "    # Process free living label and remove nans\n",
    "    df_ts = df_ts.dropna(subset=gc.columns.L_GYROSCOPE)\n",
    "        \n",
    "    # Apply threshold and filter data\n",
    "    df_ts[gc.columns.PRED_GAIT] = (df_ts[gc.columns.PRED_GAIT_PROBA] >= threshold).astype(int)\n",
    "\n",
    "    # Perform principal component analysis on the gyroscope signals to obtain the angular velocity in the\n",
    "    # direction of the swing of the arm \n",
    "    df_ts[gc.columns.VELOCITY] = pca_transform_gyroscope(\n",
    "        config=arm_activity_config,\n",
    "        df=df_ts,\n",
    "    )\n",
    "\n",
    "    # Integrate the angular velocity to obtain an estimation of the angle\n",
    "    df_ts[gc.columns.ANGLE] = compute_angle(\n",
    "        config=arm_activity_config,\n",
    "        df=df_ts,\n",
    "    )\n",
    "\n",
    "    # Remove the moving average from the angle to account for possible drift caused by the integration\n",
    "    # of noise in the angular velocity\n",
    "    df_ts[gc.columns.ANGLE] = remove_moving_average_angle(\n",
    "        config=arm_activity_config,\n",
    "        df=df_ts,\n",
    "    )\n",
    "    \n",
    "    # Filter unobserved data\n",
    "    if subject in gc.participant_ids.L_PD_IDS:\n",
    "        df_ts = df_ts[df_ts[gc.columns.ARM_LABEL] != 'Cannot assess']\n",
    "    \n",
    "    # Use only predicted gait for the subsequent steps\n",
    "    df_ts = df_ts[df_ts[gc.columns.PRED_GAIT] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Group consecutive timestamps into segments with new segments starting after a pre-specified gap\n",
    "    df_ts[gc.columns.SEGMENT_NR] = create_segments(\n",
    "        config=arm_activity_config,\n",
    "        df=df_ts\n",
    "    )\n",
    "\n",
    "    # Remove any segments that do not adhere to predetermined criteria\n",
    "    df_ts = discard_segments(\n",
    "        config=arm_activity_config,\n",
    "        df=df_ts\n",
    "    )\n",
    "\n",
    "    # Create windows of fixed length and step size from the time series\n",
    "    windowed_data = []\n",
    "\n",
    "    l_windowed_cols = [\n",
    "        gc.columns.TIME, gc.columns.FREE_LIVING_LABEL, gc.columns.ANGLE, gc.columns.VELOCITY\n",
    "        ] + arm_activity_config.l_accelerometer_cols + arm_activity_config.l_gravity_cols + arm_activity_config.l_gyroscope_cols\n",
    "    \n",
    "    if subject in gc.participant_ids.L_PD_IDS:\n",
    "        l_windowed_cols += [gc.columns.ARM_LABEL]\n",
    "\n",
    "    df_grouped = df_ts.groupby(gc.columns.SEGMENT_NR, sort=False)\n",
    "\n",
    "    for _, group in df_grouped:\n",
    "        windows = tabulate_windows(\n",
    "            config=arm_activity_config,\n",
    "            df=group,\n",
    "            columns=l_windowed_cols\n",
    "        )\n",
    "        if len(windows) > 0:  # Skip if no windows are created\n",
    "            windowed_data.append(windows)\n",
    "\n",
    "    if len(windowed_data) > 0:\n",
    "        windowed_data = np.concatenate(windowed_data, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"No windows were created from the given data.\")\n",
    "\n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "    df_features[gc.columns.TIME] = sorted(windowed_data[:, 0, l_windowed_cols.index(gc.columns.TIME)])\n",
    "\n",
    "    if subject in gc.participant_ids.L_PD_IDS:\n",
    "        df_features = pd.merge(left=df_features, right=df_ts[[gc.columns.TIME, gc.columns.PRE_OR_POST]], how='left', on=gc.columns.TIME) \n",
    "\n",
    "    # Calulate the mode of the labels\n",
    "    windowed_labels = windowed_data[:, :, l_windowed_cols.index(gc.columns.FREE_LIVING_LABEL)]\n",
    "    modes_and_counts = np.apply_along_axis(lambda x: compute_mode(x), axis=1, arr=windowed_labels)\n",
    "    modes, counts = zip(*modes_and_counts)\n",
    "\n",
    "    df_features[gc.columns.ACTIVITY_LABEL_MAJORITY_VOTING] = modes\n",
    "    df_features[gc.columns.GAIT_MAJORITY_VOTING] = [is_majority(window) for window in windowed_labels]\n",
    "\n",
    "    if subject in gc.participant_ids.L_PD_IDS:\n",
    "        windowed_labels = windowed_data[:, :, l_windowed_cols.index(gc.columns.ARM_LABEL)]\n",
    "        modes_and_counts = np.apply_along_axis(lambda x: compute_mode(x), axis=1, arr=windowed_labels)\n",
    "        modes, counts = zip(*modes_and_counts)\n",
    "\n",
    "        df_features[gc.columns.ARM_LABEL_MAJORITY_VOTING] = modes\n",
    "        df_features[gc.columns.NO_OTHER_ARM_ACTIVITY_MAJORITY_VOTING] = [is_majority(window, target=\"Gait without other behaviours or other positions\") for window in windowed_labels]\n",
    "\n",
    "    # compute statistics of the temporal domain signals\n",
    "    accel_indices = [l_windowed_cols.index(x) for x in gait_config.l_accelerometer_cols]\n",
    "    grav_indices = [l_windowed_cols.index(x) for x in gait_config.l_gravity_cols]\n",
    "    gyro_indices = [l_windowed_cols.index(x) for x in gait_config.l_gyroscope_cols]\n",
    "    idx_angle = l_windowed_cols.index(gc.columns.ANGLE)\n",
    "    idx_velocity = l_windowed_cols.index(gc.columns.VELOCITY)\n",
    "\n",
    "    accel_windowed = np.asarray(windowed_data[:, :, np.min(accel_indices):np.max(accel_indices) + 1], dtype=float)\n",
    "    grav_windowed = np.asarray(windowed_data[:, :, np.min(grav_indices):np.max(grav_indices) + 1], dtype=float)\n",
    "    gyro_windowed = np.asarray(windowed_data[:, :, np.min(gyro_indices):np.max(gyro_indices) + 1], dtype=float)\n",
    "    angle_windowed = np.asarray(windowed_data[:, :, idx_angle], dtype=float)\n",
    "    velocity_windowed = np.asarray(windowed_data[:, :, idx_velocity], dtype=float)\n",
    "\n",
    "    # angle features\n",
    "    df_features_angle = extract_angle_features(arm_activity_config, angle_windowed, velocity_windowed)\n",
    "    df_features = pd.concat([df_features, df_features_angle], axis=1)\n",
    "\n",
    "    # compute statistics of the temporal domain accelerometer signals\n",
    "    df_temporal_features = extract_temporal_domain_features(arm_activity_config, accel_windowed, grav_windowed, l_grav_stats=['mean', 'std'])\n",
    "    df_features = pd.concat([df_features, df_temporal_features], axis=1)\n",
    "\n",
    "    # transform the accelerometer and gyroscope signals from the temporal domain to the spectral domain\n",
    "    # using the fast fourier transform and extract spectral features\n",
    "    for sensor_name, windowed_sensor in zip(['accelerometer', 'gyroscope'], [accel_windowed, gyro_windowed]):\n",
    "        df_spectral_features = extract_spectral_domain_features(arm_activity_config, sensor_name, windowed_sensor)\n",
    "        df_features = pd.concat([df_features, df_spectral_features], axis=1)\n",
    "\n",
    "    file_path = os.path.join(gc.paths.PATH_ARM_ACTIVITY_FEATURES, f'{subject}_{side}.pkl')\n",
    "    df_features.to_pickle(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd-at-home-4UNzdMX4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
